WARNING 02-23 15:53:58 config.py:1155] Casting torch.bfloat16 to torch.float16.
INFO 02-23 15:53:58 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct)
INFO 02-23 15:53:59 weight_utils.py:207] Using model weights format ['*.safetensors']
INFO 02-23 15:54:03 model_runner.py:146] Loading model weights took 14.2487 GB
INFO 02-23 15:54:04 gpu_executor.py:83] # GPU blocks: 17291, # CPU blocks: 4681
INFO 02-23 15:54:06 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-23 15:54:06 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-23 15:54:14 model_runner.py:924] Graph capturing finished in 8 secs.
INFO 02-23 15:54:15 serving_chat.py:84] Using default chat template:
INFO 02-23 15:54:15 serving_chat.py:84] {%- if tools %}
INFO 02-23 15:54:15 serving_chat.py:84]     {{- '<|im_start|>system\n' }}
INFO 02-23 15:54:15 serving_chat.py:84]     {%- if messages[0]['role'] == 'system' %}
INFO 02-23 15:54:15 serving_chat.py:84]         {{- messages[0]['content'] }}
INFO 02-23 15:54:15 serving_chat.py:84]     {%- else %}
INFO 02-23 15:54:15 serving_chat.py:84]         {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}
INFO 02-23 15:54:15 serving_chat.py:84]     {%- endif %}
INFO 02-23 15:54:15 serving_chat.py:84]     {{- "\n\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}
INFO 02-23 15:54:15 serving_chat.py:84]     {%- for tool in tools %}
INFO 02-23 15:54:15 serving_chat.py:84]         {{- "\n" }}
INFO 02-23 15:54:15 serving_chat.py:84]         {{- tool | tojson }}
INFO 02-23 15:54:15 serving_chat.py:84]     {%- endfor %}
INFO 02-23 15:54:15 serving_chat.py:84]     {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n" }}
INFO 02-23 15:54:15 serving_chat.py:84] {%- else %}
INFO 02-23 15:54:15 serving_chat.py:84]     {%- if messages[0]['role'] == 'system' %}
INFO 02-23 15:54:15 serving_chat.py:84]         {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}
INFO 02-23 15:54:15 serving_chat.py:84]     {%- else %}
INFO 02-23 15:54:15 serving_chat.py:84]         {{- '<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n' }}
INFO 02-23 15:54:15 serving_chat.py:84]     {%- endif %}
INFO 02-23 15:54:15 serving_chat.py:84] {%- endif %}
INFO 02-23 15:54:15 serving_chat.py:84] {%- for message in messages %}
INFO 02-23 15:54:15 serving_chat.py:84]     {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}
INFO 02-23 15:54:15 serving_chat.py:84]         {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}
INFO 02-23 15:54:15 serving_chat.py:84]     {%- elif message.role == "assistant" %}
INFO 02-23 15:54:15 serving_chat.py:84]         {{- '<|im_start|>' + message.role }}
INFO 02-23 15:54:15 serving_chat.py:84]         {%- if message.content %}
INFO 02-23 15:54:15 serving_chat.py:84]             {{- '\n' + message.content }}
INFO 02-23 15:54:15 serving_chat.py:84]         {%- endif %}
INFO 02-23 15:54:15 serving_chat.py:84]         {%- for tool_call in message.tool_calls %}
INFO 02-23 15:54:15 serving_chat.py:84]             {%- if tool_call.function is defined %}
INFO 02-23 15:54:15 serving_chat.py:84]                 {%- set tool_call = tool_call.function %}
INFO 02-23 15:54:15 serving_chat.py:84]             {%- endif %}
INFO 02-23 15:54:15 serving_chat.py:84]             {{- '\n<tool_call>\n{"name": "' }}
INFO 02-23 15:54:15 serving_chat.py:84]             {{- tool_call.name }}
INFO 02-23 15:54:15 serving_chat.py:84]             {{- '", "arguments": ' }}
INFO 02-23 15:54:15 serving_chat.py:84]             {{- tool_call.arguments | tojson }}
INFO 02-23 15:54:15 serving_chat.py:84]             {{- '}\n</tool_call>' }}
INFO 02-23 15:54:15 serving_chat.py:84]         {%- endfor %}
INFO 02-23 15:54:15 serving_chat.py:84]         {{- '<|im_end|>\n' }}
INFO 02-23 15:54:15 serving_chat.py:84]     {%- elif message.role == "tool" %}
INFO 02-23 15:54:15 serving_chat.py:84]         {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}
INFO 02-23 15:54:15 serving_chat.py:84]             {{- '<|im_start|>user' }}
INFO 02-23 15:54:15 serving_chat.py:84]         {%- endif %}
INFO 02-23 15:54:15 serving_chat.py:84]         {{- '\n<tool_response>\n' }}
INFO 02-23 15:54:15 serving_chat.py:84]         {{- message.content }}
INFO 02-23 15:54:15 serving_chat.py:84]         {{- '\n</tool_response>' }}
INFO 02-23 15:54:15 serving_chat.py:84]         {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
INFO 02-23 15:54:15 serving_chat.py:84]             {{- '<|im_end|>\n' }}
INFO 02-23 15:54:15 serving_chat.py:84]         {%- endif %}
INFO 02-23 15:54:15 serving_chat.py:84]     {%- endif %}
INFO 02-23 15:54:15 serving_chat.py:84] {%- endfor %}
INFO 02-23 15:54:15 serving_chat.py:84] {%- if add_generation_prompt %}
INFO 02-23 15:54:15 serving_chat.py:84]     {{- '<|im_start|>assistant\n' }}
INFO 02-23 15:54:15 serving_chat.py:84] {%- endif %}
INFO 02-23 15:54:15 serving_chat.py:84] 
WARNING 02-23 15:54:16 serving_embedding.py:141] embedding_mode is False. Embedding API will not work.
INFO:     Started server process [1422104]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:18001 (Press CTRL+C to quit)
INFO 02-23 15:54:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-23 15:54:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-23 15:54:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-23 15:54:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-23 15:55:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     127.0.0.1:44982 - "GET /v1/models HTTP/1.1" 200 OK
INFO 02-23 15:55:16 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     127.0.0.1:38126 - "GET /v1/models HTTP/1.1" 200 OK
INFO 02-23 15:55:26 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-23 15:55:36 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-23 15:55:46 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-23 15:55:56 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-23 15:56:06 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [1422104]
