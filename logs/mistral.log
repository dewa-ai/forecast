WARNING 02-23 15:54:58 config.py:1155] Casting torch.bfloat16 to torch.float16.
INFO 02-23 15:54:58 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3)
INFO 02-23 15:54:59 weight_utils.py:207] Using model weights format ['*.safetensors']
INFO 02-23 15:55:03 model_runner.py:146] Loading model weights took 13.5083 GB
INFO 02-23 15:55:04 gpu_executor.py:83] # GPU blocks: 8380, # CPU blocks: 2048
INFO 02-23 15:55:05 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-23 15:55:05 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-23 15:55:12 model_runner.py:924] Graph capturing finished in 7 secs.
INFO 02-23 15:55:13 serving_chat.py:84] Using default chat template:
INFO 02-23 15:55:13 serving_chat.py:84] {%- if messages[0]["role"] == "system" %}
INFO 02-23 15:55:13 serving_chat.py:84]     {%- set system_message = messages[0]["content"] %}
INFO 02-23 15:55:13 serving_chat.py:84]     {%- set loop_messages = messages[1:] %}
INFO 02-23 15:55:13 serving_chat.py:84] {%- else %}
INFO 02-23 15:55:13 serving_chat.py:84]     {%- set loop_messages = messages %}
INFO 02-23 15:55:13 serving_chat.py:84] {%- endif %}
INFO 02-23 15:55:13 serving_chat.py:84] {%- if not tools is defined %}
INFO 02-23 15:55:13 serving_chat.py:84]     {%- set tools = none %}
INFO 02-23 15:55:13 serving_chat.py:84] {%- endif %}
INFO 02-23 15:55:13 serving_chat.py:84] {%- set user_messages = loop_messages | selectattr("role", "equalto", "user") | list %}
INFO 02-23 15:55:13 serving_chat.py:84] 
INFO 02-23 15:55:13 serving_chat.py:84] {#- This block checks for alternating user/assistant messages, skipping tool calling messages #}
INFO 02-23 15:55:13 serving_chat.py:84] {%- set ns = namespace() %}
INFO 02-23 15:55:13 serving_chat.py:84] {%- set ns.index = 0 %}
INFO 02-23 15:55:13 serving_chat.py:84] {%- for message in loop_messages %}
INFO 02-23 15:55:13 serving_chat.py:84]     {%- if not (message.role == "tool" or message.role == "tool_results" or (message.tool_calls is defined and message.tool_calls is not none)) %}
INFO 02-23 15:55:13 serving_chat.py:84]         {%- if (message["role"] == "user") != (ns.index % 2 == 0) %}
INFO 02-23 15:55:13 serving_chat.py:84]             {{- raise_exception("After the optional system message, conversation roles must alternate user/assistant/user/assistant/...") }}
INFO 02-23 15:55:13 serving_chat.py:84]         {%- endif %}
INFO 02-23 15:55:13 serving_chat.py:84]         {%- set ns.index = ns.index + 1 %}
INFO 02-23 15:55:13 serving_chat.py:84]     {%- endif %}
INFO 02-23 15:55:13 serving_chat.py:84] {%- endfor %}
INFO 02-23 15:55:13 serving_chat.py:84] 
INFO 02-23 15:55:13 serving_chat.py:84] {{- bos_token }}
INFO 02-23 15:55:13 serving_chat.py:84] {%- for message in loop_messages %}
INFO 02-23 15:55:13 serving_chat.py:84]     {%- if message["role"] == "user" %}
INFO 02-23 15:55:13 serving_chat.py:84]         {%- if tools is not none and (message == user_messages[-1]) %}
INFO 02-23 15:55:13 serving_chat.py:84]             {{- "[AVAILABLE_TOOLS] [" }}
INFO 02-23 15:55:13 serving_chat.py:84]             {%- for tool in tools %}
INFO 02-23 15:55:13 serving_chat.py:84]                 {%- set tool = tool.function %}
INFO 02-23 15:55:13 serving_chat.py:84]                 {{- '{"type": "function", "function": {' }}
INFO 02-23 15:55:13 serving_chat.py:84]                 {%- for key, val in tool.items() if key != "return" %}
INFO 02-23 15:55:13 serving_chat.py:84]                     {%- if val is string %}
INFO 02-23 15:55:13 serving_chat.py:84]                         {{- '"' + key + '": "' + val + '"' }}
INFO 02-23 15:55:13 serving_chat.py:84]                     {%- else %}
INFO 02-23 15:55:13 serving_chat.py:84]                         {{- '"' + key + '": ' + val|tojson }}
INFO 02-23 15:55:13 serving_chat.py:84]                     {%- endif %}
INFO 02-23 15:55:13 serving_chat.py:84]                     {%- if not loop.last %}
INFO 02-23 15:55:13 serving_chat.py:84]                         {{- ", " }}
INFO 02-23 15:55:13 serving_chat.py:84]                     {%- endif %}
INFO 02-23 15:55:13 serving_chat.py:84]                 {%- endfor %}
INFO 02-23 15:55:13 serving_chat.py:84]                 {{- "}}" }}
INFO 02-23 15:55:13 serving_chat.py:84]                 {%- if not loop.last %}
INFO 02-23 15:55:13 serving_chat.py:84]                     {{- ", " }}
INFO 02-23 15:55:13 serving_chat.py:84]                 {%- else %}
INFO 02-23 15:55:13 serving_chat.py:84]                     {{- "]" }}
INFO 02-23 15:55:13 serving_chat.py:84]                 {%- endif %}
INFO 02-23 15:55:13 serving_chat.py:84]             {%- endfor %}
INFO 02-23 15:55:13 serving_chat.py:84]             {{- "[/AVAILABLE_TOOLS]" }}
INFO 02-23 15:55:13 serving_chat.py:84]             {%- endif %}
INFO 02-23 15:55:13 serving_chat.py:84]         {%- if loop.last and system_message is defined %}
INFO 02-23 15:55:13 serving_chat.py:84]             {{- "[INST] " + system_message + "\n\n" + message["content"] + "[/INST]" }}
INFO 02-23 15:55:13 serving_chat.py:84]         {%- else %}
INFO 02-23 15:55:13 serving_chat.py:84]             {{- "[INST] " + message["content"] + "[/INST]" }}
INFO 02-23 15:55:13 serving_chat.py:84]         {%- endif %}
INFO 02-23 15:55:13 serving_chat.py:84]     {%- elif message.tool_calls is defined and message.tool_calls is not none %}
INFO 02-23 15:55:13 serving_chat.py:84]         {{- "[TOOL_CALLS] [" }}
INFO 02-23 15:55:13 serving_chat.py:84]         {%- for tool_call in message.tool_calls %}
INFO 02-23 15:55:13 serving_chat.py:84]             {%- set out = tool_call.function|tojson %}
INFO 02-23 15:55:13 serving_chat.py:84]             {{- out[:-1] }}
INFO 02-23 15:55:13 serving_chat.py:84]             {%- if not tool_call.id is defined or tool_call.id|length != 9 %}
INFO 02-23 15:55:13 serving_chat.py:84]                 {{- raise_exception("Tool call IDs should be alphanumeric strings with length 9!") }}
INFO 02-23 15:55:13 serving_chat.py:84]             {%- endif %}
INFO 02-23 15:55:13 serving_chat.py:84]             {{- ', "id": "' + tool_call.id + '"}' }}
INFO 02-23 15:55:13 serving_chat.py:84]             {%- if not loop.last %}
INFO 02-23 15:55:13 serving_chat.py:84]                 {{- ", " }}
INFO 02-23 15:55:13 serving_chat.py:84]             {%- else %}
INFO 02-23 15:55:13 serving_chat.py:84]                 {{- "]" + eos_token }}
INFO 02-23 15:55:13 serving_chat.py:84]             {%- endif %}
INFO 02-23 15:55:13 serving_chat.py:84]         {%- endfor %}
INFO 02-23 15:55:13 serving_chat.py:84]     {%- elif message["role"] == "assistant" %}
INFO 02-23 15:55:13 serving_chat.py:84]         {{- " " + message["content"]|trim + eos_token}}
INFO 02-23 15:55:13 serving_chat.py:84]     {%- elif message["role"] == "tool_results" or message["role"] == "tool" %}
INFO 02-23 15:55:13 serving_chat.py:84]         {%- if message.content is defined and message.content.content is defined %}
INFO 02-23 15:55:13 serving_chat.py:84]             {%- set content = message.content.content %}
INFO 02-23 15:55:13 serving_chat.py:84]         {%- else %}
INFO 02-23 15:55:13 serving_chat.py:84]             {%- set content = message.content %}
INFO 02-23 15:55:13 serving_chat.py:84]         {%- endif %}
INFO 02-23 15:55:13 serving_chat.py:84]         {{- '[TOOL_RESULTS] {"content": ' + content|string + ", " }}
INFO 02-23 15:55:13 serving_chat.py:84]         {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}
INFO 02-23 15:55:13 serving_chat.py:84]             {{- raise_exception("Tool call IDs should be alphanumeric strings with length 9!") }}
INFO 02-23 15:55:13 serving_chat.py:84]         {%- endif %}
INFO 02-23 15:55:13 serving_chat.py:84]         {{- '"call_id": "' + message.tool_call_id + '"}[/TOOL_RESULTS]' }}
INFO 02-23 15:55:13 serving_chat.py:84]     {%- else %}
INFO 02-23 15:55:13 serving_chat.py:84]         {{- raise_exception("Only user and assistant roles are supported, with the exception of an initial optional system message!") }}
INFO 02-23 15:55:13 serving_chat.py:84]     {%- endif %}
INFO 02-23 15:55:13 serving_chat.py:84] {%- endfor %}
INFO 02-23 15:55:13 serving_chat.py:84] 
WARNING 02-23 15:55:15 serving_embedding.py:141] embedding_mode is False. Embedding API will not work.
INFO:     Started server process [1422577]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:18002 (Press CTRL+C to quit)
INFO:     127.0.0.1:52718 - "GET /v1/models HTTP/1.1" 200 OK
INFO 02-23 15:55:25 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-23 15:55:35 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-23 15:55:45 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-23 15:55:55 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 02-23 15:56:05 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [1422577]
